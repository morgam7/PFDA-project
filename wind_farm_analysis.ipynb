{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wind Farm Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the Data\n",
    "\n",
    "The weather [data](https://www.met.ie/climate/available-data/historical-data) used in this project was sourced from Met Éireann, the Irish Meteorological Service. I wanted wind speed data so I had to look at each weather station's dataset to see if it had a windspeed column. This seemed endless as there were so many, until I realised that only the bigger weather stations measured windspeed and only these ones collected hourly data - which I could filter by. I ended up with 18 datasets. The goal was to ensure comprehensive coverage of significant geographical areas.\n",
    "\n",
    "Each station’s data was provided as a separate CSV file, and each file was contained within its own folder alongside metadata and licensing information. This meant I had to unzip and organise the files before processing them, which took forever. After that I tried to automate as much of the data handling that I could."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                date  ind rain  ind.1 temp  ind.2 wetb dewpt vappr rhum msl  \\\n",
      "0  13-aug-2003 01:00   -1           4           4                             \n",
      "1  13-aug-2003 02:00   -1           4           4                             \n",
      "2  13-aug-2003 03:00   -1           4           4                             \n",
      "3  13-aug-2003 04:00   -1           4           4                             \n",
      "4  13-aug-2003 05:00   -1           4           4                             \n",
      "\n",
      "   ind.3 wdsp  ind.4 wddir  \n",
      "0      7           7        \n",
      "1      7           7        \n",
      "2      7           7        \n",
      "3      7           7        \n",
      "4      7           7        \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 186744 entries, 0 to 186743\n",
      "Data columns (total 15 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   date    186744 non-null  object\n",
      " 1   ind     186744 non-null  int64 \n",
      " 2   rain    186744 non-null  object\n",
      " 3   ind.1   186744 non-null  int64 \n",
      " 4   temp    186744 non-null  object\n",
      " 5   ind.2   186744 non-null  int64 \n",
      " 6   wetb    186744 non-null  object\n",
      " 7   dewpt   186744 non-null  object\n",
      " 8   vappr   186744 non-null  object\n",
      " 9   rhum    186744 non-null  object\n",
      " 10  msl     186744 non-null  object\n",
      " 11  ind.3   186744 non-null  int64 \n",
      " 12  wdsp    186744 non-null  object\n",
      " 13  ind.4   186744 non-null  int64 \n",
      " 14  wddir   186744 non-null  object\n",
      "dtypes: int64(5), object(10)\n",
      "memory usage: 21.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/hly275/hly275.csv', skiprows = 17, low_memory=False)\n",
    "print(df.head())\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating Skipping Metadata\n",
    "\n",
    "The weather datasets I’m working with include metadata at the top, with the actual data starting on different rows in each file. Manually checking which row the header starts on for every file and then using `skiprows` would take ages and isn’t practical.\n",
    "\n",
    "To automate this, I wrote a function that detects the header row automatically. It looks for specific keywords ('date' and 'wdsp') that always appear in the header column names and don't appear together on a row in the metadata. Once it finds the right row, it skips all the metadata and reads the data directly from the header onwards. This also confirmed that all the datasets had a windspeed column. This [pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) helped.\n",
    "\n",
    "Another issue I had was the name of the weather station is contained in the metadata. This information is essential for analysis, but it isn’t part of the main data table. I needed a way to extract the station name from the metadata and add it as a column in the dataset. I wasn't going to do this manually.\n",
    "\n",
    "So I wrote a function to extract the station name from the metadata. Each file contains a line like: `Station Name: MACE HEAD` at the start, so the function [searches for that specific line](https://www.statology.org/pandas-query-startswith/) and [grabs the station name](https://www.w3schools.com/python/ref_string_split.asp). To ensure consistency, the name is converted to uppercase and added as a new column `station` in the cleaned dataset. This process ensures that each row of data is tagged with the correct station name for future analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_header_row(file_path):\n",
    "    \"\"\"\n",
    "    Detects the row number where the header starts based on the presence of 'date' and 'wdsp'.\n",
    "    \"\"\"\n",
    "    row_number = 0  # Start counting rows\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:  # Read the file line by line\n",
    "            if \"date\" in line.lower() and \"wdsp\" in line.lower(): # .lower makes it case insensitve just in case\n",
    "                return row_number  # Return the current row number as the header row. This is the row number I can use for skiprows in fucntion below\n",
    "            row_number += 1  # row counter goes up by one\n",
    "    return None  # Return None if no valid header is found\n",
    "\n",
    "\n",
    "def extract_station_name(file_path):\n",
    "    \"\"\"\n",
    "    Extracts the station name from the first line of each csv file that starts with 'Station Name:'.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(\"Station Name:\"):\n",
    "                # Extract the part after \"Station Name:\" and strip any whitespace\n",
    "                return line.split(\"Station Name:\")[1].strip().upper()  # Ensure the name is uppercase\n",
    "    return None  # Return None if no station name is found\n",
    "\n",
    "\n",
    "def read_clean_csv(file_path):\n",
    "    \"\"\"\n",
    "    Reads a CSV file, skips metadata rows, and adds a 'station' column.\n",
    "    \"\"\"\n",
    "    # Extract the station name from the metadata\n",
    "    station_name = extract_station_name(file_path)\n",
    "\n",
    "    # Detect the header row\n",
    "    header_row = detect_header_row(file_path)\n",
    "    if header_row is None:\n",
    "        raise ValueError(f\"No valid header found in {file_path}\")\n",
    "\n",
    "    # Read the data starting from the header row\n",
    "    df = pd.read_csv(file_path, skiprows=header_row, low_memory=False)\n",
    "    \n",
    "    # Add Station Name as a column\n",
    "    df['station'] = station_name\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing fucntions with one of the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Station Name: MACE HEAD\n",
      "Detected Header Row: 17\n",
      "                date  ind rain  ind.1 temp  ind.2 wetb dewpt vappr rhum msl  \\\n",
      "0  13-aug-2003 01:00   -1           4           4                             \n",
      "1  13-aug-2003 02:00   -1           4           4                             \n",
      "2  13-aug-2003 03:00   -1           4           4                             \n",
      "3  13-aug-2003 04:00   -1           4           4                             \n",
      "4  13-aug-2003 05:00   -1           4           4                             \n",
      "\n",
      "   ind.3 wdsp  ind.4 wddir    station  \n",
      "0      7           7        MACE HEAD  \n",
      "1      7           7        MACE HEAD  \n",
      "2      7           7        MACE HEAD  \n",
      "3      7           7        MACE HEAD  \n",
      "4      7           7        MACE HEAD  \n"
     ]
    }
   ],
   "source": [
    "file_path = \"data/hly275/hly275.csv\"  # Replace with the path to one of your files\n",
    "\n",
    "station_name = extract_station_name(file_path)\n",
    "print(f\"Extracted Station Name: {station_name}\")\n",
    "\n",
    "header_row = detect_header_row(file_path)\n",
    "print(f\"Detected Header Row: {header_row}\")\n",
    "\n",
    "cleaned_data = read_clean_csv(file_path)\n",
    "print(cleaned_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making one Big Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like the functions work!\n",
    "\n",
    "I wanted to run the functions with each of the 18 CSV files I had. So I needed to create a list with each of the file paths. But I didn't want to do this manually. The [glob module](https://docs.python.org/3/library/glob.html) helped out here. I was able to search the data directory for the CSV files, and it even searched subdirectories within the data directory using its recursive search function [the `**` pattern](https://www.geeksforgeeks.org/how-to-use-glob-function-to-find-files-recursively-in-python/). So I could keep each CSV file in its own folder alongside its licence and other metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data\\\\hly1075\\\\hly1075.csv', 'data\\\\hly1175\\\\hly1175.csv', 'data\\\\hly1375\\\\hly1375.csv', 'data\\\\hly1475\\\\hly1475.csv', 'data\\\\hly1575\\\\hly1575.csv', 'data\\\\hly1775\\\\hly1775.csv', 'data\\\\hly1875\\\\hly1875.csv', 'data\\\\hly1975\\\\hly1975.csv', 'data\\\\hly2075\\\\hly2075.csv', 'data\\\\hly2175\\\\hly2175.csv', 'data\\\\hly2275\\\\hly2275.csv', 'data\\\\hly2375\\\\hly2375.csv', 'data\\\\hly275\\\\hly275.csv', 'data\\\\hly375\\\\hly375.csv', 'data\\\\hly575\\\\hly575.csv', 'data\\\\hly675\\\\hly675.csv', 'data\\\\hly775\\\\hly775.csv', 'data\\\\hly875\\\\hly875.csv']\n",
      "Number of files found: 18\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# The directory I want to seach and also its subdirectories\n",
    "data_path = \"data/**/*.csv\"  \n",
    "\n",
    "# Get a list of all CSV files in the directory and its subdirectories\n",
    "file_list = glob.glob(data_path, recursive=True)\n",
    "\n",
    "print(file_list)  # Check the list of file paths\n",
    "print(f\"Number of files found: {len(file_list)}\") # Checking I got all of them\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next bit of code processes all the weather station files in the `file_list` by running each through the `read_clean_csv` function, which skips metadata and adds a `station` column to identify the source. The processed DataFrames are stored in the `all_data` list and then combined into a single dataset using [concat](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#concatenating-objects), creating `my_big_dataset` with all the data in one place. I ensured there would be a continuous index with the `ignore_index=True` parameter. \n",
    "\n",
    "Finally, the code checks the number of processed files by counting the entries in `all_data`, confirming that all files were successfully handled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed files: 18\n",
      "Dataset saved as 'my_big_dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "# Process each file in the file_list using the read_clean_csv function\n",
    "all_data = [read_clean_csv(file) for file in file_list]\n",
    "\n",
    "# Combine all processed DataFrames into one\n",
    "my_big_dataset = pd.concat(all_data, ignore_index=True,)\n",
    "\n",
    "# Checking all 18 files were processed\n",
    "print(f\"Number of processed files: {len(all_data)}\")\n",
    "\n",
    "#Saving as CSV file and checking it was saved\n",
    "my_big_dataset.to_csv(\"my_big_dataset.csv\", index=False)\n",
    "print(\"Dataset saved as 'my_big_dataset.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning My Big Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ended up with a dataset that had over 5 million rows. So I printed it out and sat down at the kitchen table with a pencil and ruler so I could check each row for any issues. Just kidding! I didn't do that. Instead I used some handy code to figure out what kind of data I was dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      date  ind rain  ind.1  temp  ind.2  wetb dewpt vappr  \\\n",
      "0        01-dec-1955 01:00    0  0.0      0  10.7      0  10.0   9.4  11.8   \n",
      "1        01-dec-1955 02:00    0  2.9      0   9.8      0   9.7  10.0  12.0   \n",
      "2        01-dec-1955 03:00    0  3.8      0   9.7      0   9.5   9.4  11.7   \n",
      "3        01-dec-1955 04:00    0  0.8      0   9.8      0   9.7   9.4  11.9   \n",
      "4        01-dec-1955 05:00    0  0.3      0   8.9      0   8.7   8.3  11.1   \n",
      "...                    ...  ...  ...    ...   ...    ...   ...   ...   ...   \n",
      "5745370  30-nov-2024 20:00    0  0.0      0  13.5      0  12.7  12.0  14.0   \n",
      "5745371  30-nov-2024 21:00    0  0.0      0  13.5      0  12.7  12.0  14.0   \n",
      "5745372  30-nov-2024 22:00    0  4.1      0  12.1      0  11.7  11.4  13.5   \n",
      "5745373  30-nov-2024 23:00    0  0.0      0  12.3      0  11.9  11.6  13.7   \n",
      "5745374  01-dec-2024 00:00    0  0.0      0  12.3      0  11.8  11.4  13.4   \n",
      "\n",
      "        rhum  ... wdsp  ind.4 wddir       station   ww    w  sun  vis clht  \\\n",
      "0         91  ...   16      1   170  ROCHES POINT  NaN  NaN  NaN  NaN  NaN   \n",
      "1         99  ...   11      1   190  ROCHES POINT  NaN  NaN  NaN  NaN  NaN   \n",
      "2         97  ...    9      1   160  ROCHES POINT  NaN  NaN  NaN  NaN  NaN   \n",
      "3         98  ...    5      1   140  ROCHES POINT  NaN  NaN  NaN  NaN  NaN   \n",
      "4         97  ...   12      1   330  ROCHES POINT  NaN  NaN  NaN  NaN  NaN   \n",
      "...      ...  ...  ...    ...   ...           ...  ...  ...  ...  ...  ...   \n",
      "5745370   90  ...   14      2   180     MULLINGAR  NaN  NaN  NaN  NaN  NaN   \n",
      "5745371   90  ...   12      2   190     MULLINGAR  NaN  NaN  NaN  NaN  NaN   \n",
      "5745372   95  ...   12      2   240     MULLINGAR  NaN  NaN  NaN  NaN  NaN   \n",
      "5745373   95  ...    5      2   210     MULLINGAR  NaN  NaN  NaN  NaN  NaN   \n",
      "5745374   93  ...    6      2   200     MULLINGAR  NaN  NaN  NaN  NaN  NaN   \n",
      "\n",
      "        clamt  \n",
      "0         NaN  \n",
      "1         NaN  \n",
      "2         NaN  \n",
      "3         NaN  \n",
      "4         NaN  \n",
      "...       ...  \n",
      "5745370   NaN  \n",
      "5745371   NaN  \n",
      "5745372   NaN  \n",
      "5745373   NaN  \n",
      "5745374   NaN  \n",
      "\n",
      "[5745375 rows x 22 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5745375 entries, 0 to 5745374\n",
      "Data columns (total 22 columns):\n",
      " #   Column   Dtype \n",
      "---  ------   ----- \n",
      " 0   date     object\n",
      " 1   ind      int64 \n",
      " 2   rain     object\n",
      " 3   ind.1    int64 \n",
      " 4   temp     object\n",
      " 5   ind.2    int64 \n",
      " 6   wetb     object\n",
      " 7   dewpt    object\n",
      " 8   vappr    object\n",
      " 9   rhum     object\n",
      " 10  msl      object\n",
      " 11  ind.3    int64 \n",
      " 12  wdsp     object\n",
      " 13  ind.4    int64 \n",
      " 14  wddir    object\n",
      " 15  station  object\n",
      " 16  ww       object\n",
      " 17  w        object\n",
      " 18  sun      object\n",
      " 19  vis      object\n",
      " 20  clht     object\n",
      " 21  clamt    object\n",
      "dtypes: int64(5), object(17)\n",
      "memory usage: 964.3+ MB\n",
      "None\n",
      "date             0\n",
      "ind              0\n",
      "rain             0\n",
      "ind.1            0\n",
      "temp             0\n",
      "ind.2            0\n",
      "wetb             0\n",
      "dewpt            0\n",
      "vappr            0\n",
      "rhum             0\n",
      "msl              0\n",
      "ind.3            0\n",
      "wdsp             0\n",
      "ind.4            0\n",
      "wddir            0\n",
      "station          0\n",
      "ww         3828149\n",
      "w          3828149\n",
      "sun        3828149\n",
      "vis        3828149\n",
      "clht       3828149\n",
      "clamt      3828149\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(my_big_dataset)\n",
    "\n",
    "# Checking the structure of the data\n",
    "print(my_big_dataset.info())\n",
    "\n",
    "# Check for missing values\n",
    "print(my_big_dataset.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Up Missing Values\n",
    "\n",
    "I focused on cleaning up the missing values in the dataset. Most of the original datasets had 16 columns, but a few included 5 additional columns. This introduced missing values when the datasets were merged, so it made sense that the last five columns had nearly 4 million missing values. Fortunately, `pandas.concat` [automatically converted](https://pyimagesearch.com/2024/05/07/using-pandas-concat-pd-concat/) these blanks into proper missing values (`NaN`), which were easy to handle during analysis.\n",
    "\n",
    "However, when I inspected the other columns, `pandas` was reporting 0 missing values, which didn’t match what I saw when scrolling through `my_big_dataset.csv`. The issue was that some missing values were misrepresented as empty strings. They weren’t being recognised as missing values but as blank text, and this would cause problems during analysis. \n",
    "\n",
    "Another related issue was that most of the columns containing integers were [being read as `object` type](https://www.geeksforgeeks.org/pandas-detect-mixed-data-types-and-fix-it/). This might have happened because `pandas` was trying to handle the mix of integers and empty strings in those columns. \n",
    "\n",
    "To fix this, I wanted to replace all empty strings with `NaN` using [`replace`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html). But I needed to find out first how exactly the missing values were being represented.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['16' '11' '9' '5' '12' '15' '13' '14' '7' '4' '3' '6' '1' '0' '21' '19'\n",
      " '23' '22' '20' '8' '2' '17' '18' '10' '25' '29' '26' '24' '27' '28' '30'\n",
      " '31' '36' '32' '33' '34' '40' '41' '42' '38' '35' '46' '48' '44' '37'\n",
      " '39' '43' '47' '50' '53' '54' '45' '51' '55' '49' '52' '59' '56' ' ' '62'\n",
      " '60' '57' '63' '61' '58' '97']\n"
     ]
    }
   ],
   "source": [
    "# Check unique values in a column\n",
    "print(my_big_dataset['wdsp'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the missing values were being represented as \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date             0\n",
      "ind              0\n",
      "rain        110833\n",
      "ind.1            0\n",
      "temp         30409\n",
      "ind.2            0\n",
      "wetb         42486\n",
      "dewpt        41501\n",
      "vappr       184823\n",
      "rhum        163718\n",
      "msl          71928\n",
      "ind.3            0\n",
      "wdsp         83644\n",
      "ind.4            0\n",
      "wddir        95840\n",
      "station          0\n",
      "ww         4178060\n",
      "w          4178059\n",
      "sun        4177877\n",
      "vis        4178224\n",
      "clht       4178129\n",
      "clamt      4178129\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5745375 entries, 0 to 5745374\n",
      "Data columns (total 22 columns):\n",
      " #   Column   Dtype \n",
      "---  ------   ----- \n",
      " 0   date     object\n",
      " 1   ind      int64 \n",
      " 2   rain     object\n",
      " 3   ind.1    int64 \n",
      " 4   temp     object\n",
      " 5   ind.2    int64 \n",
      " 6   wetb     object\n",
      " 7   dewpt    object\n",
      " 8   vappr    object\n",
      " 9   rhum     object\n",
      " 10  msl      object\n",
      " 11  ind.3    int64 \n",
      " 12  wdsp     object\n",
      " 13  ind.4    int64 \n",
      " 14  wddir    object\n",
      " 15  station  object\n",
      " 16  ww       object\n",
      " 17  w        object\n",
      " 18  sun      object\n",
      " 19  vis      object\n",
      " 20  clht     object\n",
      " 21  clamt    object\n",
      "dtypes: int64(5), object(17)\n",
      "memory usage: 964.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Replace all empty strings with NaN in the dataset\n",
    "my_big_dataset.replace(\" \", np.nan, inplace=True)\n",
    "\n",
    "# Verify the change by checking for missing values\n",
    "print(my_big_dataset.isnull().sum())\n",
    "print(my_big_dataset.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was still getting the rows with ints and floats being read as objects. I wanted them to be numeric. So I decided to [convert](https://pandas.pydata.org/docs/reference/api/pandas.to_numeric.html) them. \n",
    "\n",
    "I also converted the `date` column to a proper datetime format for easier analysis and consistency. It was taking forever to convert to datetime. I stopped running the code after 10mins. So I went googling and I found [this thread](https://stackoverflow.com/questions/32034689/why-is-pandas-to-datetime-slow-for-non-standard-time-format-such-as-2014-12-31) that expained the it would be quicker if I gave pandas the format for the date column - so it wouldn't have to go looking for it and it was much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5745375 entries, 0 to 5745374\n",
      "Data columns (total 22 columns):\n",
      " #   Column   Dtype         \n",
      "---  ------   -----         \n",
      " 0   date     datetime64[ns]\n",
      " 1   ind      int64         \n",
      " 2   rain     float64       \n",
      " 3   ind.1    int64         \n",
      " 4   temp     float64       \n",
      " 5   ind.2    int64         \n",
      " 6   wetb     float64       \n",
      " 7   dewpt    float64       \n",
      " 8   vappr    float64       \n",
      " 9   rhum     float64       \n",
      " 10  msl      float64       \n",
      " 11  ind.3    int64         \n",
      " 12  wdsp     float64       \n",
      " 13  ind.4    int64         \n",
      " 14  wddir    float64       \n",
      " 15  station  object        \n",
      " 16  ww       float64       \n",
      " 17  w        float64       \n",
      " 18  sun      float64       \n",
      " 19  vis      float64       \n",
      " 20  clht     float64       \n",
      " 21  clamt    float64       \n",
      "dtypes: datetime64[ns](1), float64(15), int64(5), object(1)\n",
      "memory usage: 964.3+ MB\n",
      "None\n",
      "date             0\n",
      "ind              0\n",
      "rain        110833\n",
      "ind.1            0\n",
      "temp         30409\n",
      "ind.2            0\n",
      "wetb         42486\n",
      "dewpt        41501\n",
      "vappr       184823\n",
      "rhum        163718\n",
      "msl          71928\n",
      "ind.3            0\n",
      "wdsp         83644\n",
      "ind.4            0\n",
      "wddir        95840\n",
      "station          0\n",
      "ww         4178060\n",
      "w          4178059\n",
      "sun        4177877\n",
      "vis        4178224\n",
      "clht       4178129\n",
      "clamt      4178129\n",
      "dtype: int64\n",
      "date       datetime64[ns]\n",
      "ind                 int64\n",
      "rain              float64\n",
      "ind.1               int64\n",
      "temp              float64\n",
      "ind.2               int64\n",
      "wetb              float64\n",
      "dewpt             float64\n",
      "vappr             float64\n",
      "rhum              float64\n",
      "msl               float64\n",
      "ind.3               int64\n",
      "wdsp              float64\n",
      "ind.4               int64\n",
      "wddir             float64\n",
      "station            object\n",
      "ww                float64\n",
      "w                 float64\n",
      "sun               float64\n",
      "vis               float64\n",
      "clht              float64\n",
      "clamt             float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Converting all columns excpept 'data' and 'station' to numeric\n",
    "columns_to_convert = [col for col in my_big_dataset.columns if col not in ['station', 'date']]\n",
    "my_big_dataset[columns_to_convert] = my_big_dataset[columns_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Convert the date column to datetime\n",
    "my_big_dataset['date'] = pd.to_datetime(my_big_dataset['date'], format='%d-%b-%Y %H:%M')\n",
    "\n",
    "# Check the cleaned data\n",
    "print(my_big_dataset.info())\n",
    "print(my_big_dataset.isnull().sum())\n",
    "print(my_big_dataset.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
