{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wind Farm Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the Data\n",
    "\n",
    "The weather [data](https://www.met.ie/climate/available-data/historical-data) used in this project was sourced from Met Éireann, the Irish Meteorological Service. I wanted wind speed data so I had to look at each weather station's dataset to see if it had a windspeed column. This seemed endless as there were so many, until I realised that only the bigger weather stations measured windspeed and only these ones collected hourly data - which I could filter by. I ended up with 18 datasets. The goal was to ensure comprehensive coverage of significant geographical areas.\n",
    "\n",
    "Each station’s data was provided as a separate CSV file, and each file was contained within its own folder alongside metadata and licensing information. This meant I had to unzip and organise the files before processing them, which was a bit annoying. After that I tried to automate as much of the data handling that I could."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                date  ind rain  ind.1 temp  ind.2 wetb dewpt vappr rhum msl  \\\n",
      "0  13-aug-2003 01:00   -1           4           4                             \n",
      "1  13-aug-2003 02:00   -1           4           4                             \n",
      "2  13-aug-2003 03:00   -1           4           4                             \n",
      "3  13-aug-2003 04:00   -1           4           4                             \n",
      "4  13-aug-2003 05:00   -1           4           4                             \n",
      "\n",
      "   ind.3 wdsp  ind.4 wddir  \n",
      "0      7           7        \n",
      "1      7           7        \n",
      "2      7           7        \n",
      "3      7           7        \n",
      "4      7           7        \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 186744 entries, 0 to 186743\n",
      "Data columns (total 15 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   date    186744 non-null  object\n",
      " 1   ind     186744 non-null  int64 \n",
      " 2   rain    186744 non-null  object\n",
      " 3   ind.1   186744 non-null  int64 \n",
      " 4   temp    186744 non-null  object\n",
      " 5   ind.2   186744 non-null  int64 \n",
      " 6   wetb    186744 non-null  object\n",
      " 7   dewpt   186744 non-null  object\n",
      " 8   vappr   186744 non-null  object\n",
      " 9   rhum    186744 non-null  object\n",
      " 10  msl     186744 non-null  object\n",
      " 11  ind.3   186744 non-null  int64 \n",
      " 12  wdsp    186744 non-null  object\n",
      " 13  ind.4   186744 non-null  int64 \n",
      " 14  wddir   186744 non-null  object\n",
      "dtypes: int64(5), object(10)\n",
      "memory usage: 21.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/hly275/hly275.csv', skiprows = 17, low_memory=False)\n",
    "print(df.head())\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating Skipping Metadata\n",
    "\n",
    "The weather datasets I’m working with include metadata at the top, with the actual data starting on different rows in each file. Manually checking which row the header starts on for every file and then using `skiprows` would take ages and isn’t practical.\n",
    "\n",
    "To automate this, I wrote a function that detects the header row automatically. It looks for specific keywords ('date' and 'wdsp') that always appear in the header column names and don't appear together on a row in the metadata. Once it finds the right row, it skips all the metadata and reads the data directly from the header onwards. This also confirmed that all the datasets had a windspeed column. This [pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) helped.\n",
    "\n",
    "Another issue I had was the name of the weather station is contained in the metadata. This information is essential for analysis, but it isn’t part of the main data table. I needed a way to extract the station name from the metadata and add it as a column in the dataset. I wasn't going to do this manually.\n",
    "\n",
    "So I wrote a function to extract the station name from the metadata. Each file contains a line like: `Station Name: MACE HEAD` at the start, so the function [searches for that specific line](https://www.statology.org/pandas-query-startswith/) and [grabs the station name](https://www.w3schools.com/python/ref_string_split.asp). To ensure consistency, the name is converted to uppercase and added as a new column `station` in the cleaned dataset. This process ensures that each row of data is tagged with the correct station name for future analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_header_row(file_path):\n",
    "    \"\"\"\n",
    "    Detects the row number where the header starts based on the presence of 'date' and 'wdsp'.\n",
    "    \"\"\"\n",
    "    row_number = 0  # Start counting rows\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:  # Read the file line by line\n",
    "            if \"date\" in line.lower() and \"wdsp\" in line.lower(): # .lower makes it case insensitve just in case\n",
    "                return row_number  # Return the current row number as the header row. This is the row number I can use for skiprows in fucntion below\n",
    "            row_number += 1  # row counter goes up by one\n",
    "    return None  # Return None if no valid header is found\n",
    "\n",
    "\n",
    "def extract_station_name(file_path):\n",
    "    \"\"\"\n",
    "    Extracts the station name from the first line of each csv file that starts with 'Station Name:'.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(\"Station Name:\"):\n",
    "                # Extract the part after \"Station Name:\" and strip any whitespace\n",
    "                return line.split(\"Station Name:\")[1].strip().upper()  # Ensure the name is uppercase\n",
    "    return None  # Return None if no station name is found\n",
    "\n",
    "\n",
    "def read_clean_csv(file_path):\n",
    "    \"\"\"\n",
    "    Reads a CSV file, skips metadata rows, and adds a 'station' column.\n",
    "    \"\"\"\n",
    "    # Extract the station name from the metadata\n",
    "    station_name = extract_station_name(file_path)\n",
    "\n",
    "    # Detect the header row\n",
    "    header_row = detect_header_row(file_path)\n",
    "    if header_row is None:\n",
    "        raise ValueError(f\"No valid header found in {file_path}\")\n",
    "\n",
    "    # Read the data starting from the header row\n",
    "    df = pd.read_csv(file_path, skiprows=header_row, low_memory=False)\n",
    "    \n",
    "    # Add Station Name as a column\n",
    "    df['station'] = station_name\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing fucntions with one of the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Station Name: MACE HEAD\n",
      "Detected Header Row: 17\n",
      "                date  ind rain  ind.1 temp  ind.2 wetb dewpt vappr rhum msl  \\\n",
      "0  13-aug-2003 01:00   -1           4           4                             \n",
      "1  13-aug-2003 02:00   -1           4           4                             \n",
      "2  13-aug-2003 03:00   -1           4           4                             \n",
      "3  13-aug-2003 04:00   -1           4           4                             \n",
      "4  13-aug-2003 05:00   -1           4           4                             \n",
      "\n",
      "   ind.3 wdsp  ind.4 wddir    station  \n",
      "0      7           7        MACE HEAD  \n",
      "1      7           7        MACE HEAD  \n",
      "2      7           7        MACE HEAD  \n",
      "3      7           7        MACE HEAD  \n",
      "4      7           7        MACE HEAD  \n"
     ]
    }
   ],
   "source": [
    "file_path = \"data/hly275/hly275.csv\"  # Replace with the path to one of your files\n",
    "\n",
    "station_name = extract_station_name(file_path)\n",
    "print(f\"Extracted Station Name: {station_name}\")\n",
    "\n",
    "header_row = detect_header_row(file_path)\n",
    "print(f\"Detected Header Row: {header_row}\")\n",
    "\n",
    "cleaned_data = read_clean_csv(file_path)\n",
    "print(cleaned_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making one Big Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like the functions work!\n",
    "\n",
    "I wanted to run the functions with each of the 18 CSV files I had. So I needed to create a list with each of the file paths. But I didn't want to do this manually. The [glob module](https://docs.python.org/3/library/glob.html) helped out here. I was able to search the data directory for the CSV files, and it even searched subdirectories within the data directory using its recursive search function [the `**` pattern](https://www.geeksforgeeks.org/how-to-use-glob-function-to-find-files-recursively-in-python/). So I could keep each CSV file in its own folder alongside its licence and other metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data\\\\hly1075\\\\hly1075.csv', 'data\\\\hly1175\\\\hly1175.csv', 'data\\\\hly1375\\\\hly1375.csv', 'data\\\\hly1475\\\\hly1475.csv', 'data\\\\hly1575\\\\hly1575.csv', 'data\\\\hly1775\\\\hly1775.csv', 'data\\\\hly1875\\\\hly1875.csv', 'data\\\\hly1975\\\\hly1975.csv', 'data\\\\hly2075\\\\hly2075.csv', 'data\\\\hly2175\\\\hly2175.csv', 'data\\\\hly2275\\\\hly2275.csv', 'data\\\\hly2375\\\\hly2375.csv', 'data\\\\hly275\\\\hly275.csv', 'data\\\\hly375\\\\hly375.csv', 'data\\\\hly575\\\\hly575.csv', 'data\\\\hly675\\\\hly675.csv', 'data\\\\hly775\\\\hly775.csv', 'data\\\\hly875\\\\hly875.csv']\n",
      "Number of files found: 18\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# The directory I want to seach and also its subdirectories\n",
    "data_path = \"data/**/*.csv\"  \n",
    "\n",
    "# Get a list of all CSV files in the directory and its subdirectories\n",
    "file_list = glob.glob(data_path, recursive=True)\n",
    "\n",
    "print(file_list)  # Check the list of file paths\n",
    "print(f\"Number of files found: {len(file_list)}\") # Checking I got all of them\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next bit of code processes all the weather station files in the `file_list` by running each through the `read_clean_csv` function, which skips metadata and adds a `station` column to identify the source. The processed DataFrames are stored in the `all_data` list and then combined into a single dataset using [pd.concat](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#concatenating-objects), creating `my_big_dataset` with all the data in one place. I ensured there would be a continuous index with the `ignore_index=True` parameter. \n",
    "\n",
    "Finally, the code checks the number of processed files by counting the entries in `all_data`, confirming that all files were successfully handled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      date  ind rain  ind.1  temp  ind.2  wetb dewpt vappr  \\\n",
      "0        01-dec-1955 01:00    0  0.0      0  10.7      0  10.0   9.4  11.8   \n",
      "1        01-dec-1955 02:00    0  2.9      0   9.8      0   9.7  10.0  12.0   \n",
      "2        01-dec-1955 03:00    0  3.8      0   9.7      0   9.5   9.4  11.7   \n",
      "3        01-dec-1955 04:00    0  0.8      0   9.8      0   9.7   9.4  11.9   \n",
      "4        01-dec-1955 05:00    0  0.3      0   8.9      0   8.7   8.3  11.1   \n",
      "...                    ...  ...  ...    ...   ...    ...   ...   ...   ...   \n",
      "5745370  30-nov-2024 20:00    0  0.0      0  13.5      0  12.7  12.0  14.0   \n",
      "5745371  30-nov-2024 21:00    0  0.0      0  13.5      0  12.7  12.0  14.0   \n",
      "5745372  30-nov-2024 22:00    0  4.1      0  12.1      0  11.7  11.4  13.5   \n",
      "5745373  30-nov-2024 23:00    0  0.0      0  12.3      0  11.9  11.6  13.7   \n",
      "5745374  01-dec-2024 00:00    0  0.0      0  12.3      0  11.8  11.4  13.4   \n",
      "\n",
      "        rhum  ... wdsp  ind.4 wddir       station   ww    w  sun  vis clht  \\\n",
      "0         91  ...   16      1   170  ROCHES POINT  NaN  NaN  NaN  NaN  NaN   \n",
      "1         99  ...   11      1   190  ROCHES POINT  NaN  NaN  NaN  NaN  NaN   \n",
      "2         97  ...    9      1   160  ROCHES POINT  NaN  NaN  NaN  NaN  NaN   \n",
      "3         98  ...    5      1   140  ROCHES POINT  NaN  NaN  NaN  NaN  NaN   \n",
      "4         97  ...   12      1   330  ROCHES POINT  NaN  NaN  NaN  NaN  NaN   \n",
      "...      ...  ...  ...    ...   ...           ...  ...  ...  ...  ...  ...   \n",
      "5745370   90  ...   14      2   180     MULLINGAR  NaN  NaN  NaN  NaN  NaN   \n",
      "5745371   90  ...   12      2   190     MULLINGAR  NaN  NaN  NaN  NaN  NaN   \n",
      "5745372   95  ...   12      2   240     MULLINGAR  NaN  NaN  NaN  NaN  NaN   \n",
      "5745373   95  ...    5      2   210     MULLINGAR  NaN  NaN  NaN  NaN  NaN   \n",
      "5745374   93  ...    6      2   200     MULLINGAR  NaN  NaN  NaN  NaN  NaN   \n",
      "\n",
      "        clamt  \n",
      "0         NaN  \n",
      "1         NaN  \n",
      "2         NaN  \n",
      "3         NaN  \n",
      "4         NaN  \n",
      "...       ...  \n",
      "5745370   NaN  \n",
      "5745371   NaN  \n",
      "5745372   NaN  \n",
      "5745373   NaN  \n",
      "5745374   NaN  \n",
      "\n",
      "[5745375 rows x 22 columns]\n",
      "Number of processed files: 18\n"
     ]
    }
   ],
   "source": [
    "# Process each file in the file_list using the read_clean_csv function\n",
    "all_data = [read_clean_csv(file) for file in file_list]\n",
    "\n",
    "# Combine all processed DataFrames into one\n",
    "my_big_dataset = pd.concat(all_data, ignore_index=True,)\n",
    "\n",
    "# Checking the combined dataset\n",
    "print(my_big_dataset)\n",
    "\n",
    "# Checking all 18 files were processed\n",
    "print(f\"Number of processed files: {len(all_data)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
