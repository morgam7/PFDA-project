{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wind Farm Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the Data\n",
    "\n",
    "The weather [data](https://www.met.ie/climate/available-data/historical-data) used in this project was sourced from Met Éireann, the Irish Meteorological Service. I wanted wind speed data so I had to look at each weather station's dataset to see if it had a windspeed column. This seemed endless as there were so many, until I realised that only the bigger weather stations measured windspeed and only these ones collected hourly data - which I could filter by. I ended up with 18 datasets. The goal was to ensure comprehensive coverage of significant geographical areas.\n",
    "\n",
    "Each station’s data was provided as a separate CSV file, and each file was contained within its own folder alongside metadata and licensing information. This meant I had to unzip and organise the files before processing them, which took forever. After that I tried to automate as much of the data handling that I could."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                date  ind rain  ind.1 temp  ind.2 wetb dewpt vappr rhum msl  \\\n",
      "0  13-aug-2003 01:00   -1           4           4                             \n",
      "1  13-aug-2003 02:00   -1           4           4                             \n",
      "2  13-aug-2003 03:00   -1           4           4                             \n",
      "3  13-aug-2003 04:00   -1           4           4                             \n",
      "4  13-aug-2003 05:00   -1           4           4                             \n",
      "\n",
      "   ind.3 wdsp  ind.4 wddir  \n",
      "0      7           7        \n",
      "1      7           7        \n",
      "2      7           7        \n",
      "3      7           7        \n",
      "4      7           7        \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 186744 entries, 0 to 186743\n",
      "Data columns (total 15 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   date    186744 non-null  object\n",
      " 1   ind     186744 non-null  int64 \n",
      " 2   rain    186744 non-null  object\n",
      " 3   ind.1   186744 non-null  int64 \n",
      " 4   temp    186744 non-null  object\n",
      " 5   ind.2   186744 non-null  int64 \n",
      " 6   wetb    186744 non-null  object\n",
      " 7   dewpt   186744 non-null  object\n",
      " 8   vappr   186744 non-null  object\n",
      " 9   rhum    186744 non-null  object\n",
      " 10  msl     186744 non-null  object\n",
      " 11  ind.3   186744 non-null  int64 \n",
      " 12  wdsp    186744 non-null  object\n",
      " 13  ind.4   186744 non-null  int64 \n",
      " 14  wddir   186744 non-null  object\n",
      "dtypes: int64(5), object(10)\n",
      "memory usage: 21.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/hly275/hly275.csv', skiprows = 17, low_memory=False)\n",
    "print(df.head())\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating Skipping Metadata\n",
    "\n",
    "The weather datasets I’m working with include metadata at the top, with the actual data starting on different rows in each file. Manually checking which row the header starts on for every file and then using `skiprows` would take ages and isn’t practical.\n",
    "\n",
    "To automate this, I wrote a function that detects the header row automatically. It looks for specific keywords ('date' and 'wdsp') that always appear in the header column names and don't appear together on a row in the metadata. Once it finds the right row, it skips all the metadata and reads the data directly from the header onwards. This also confirmed that all the datasets had a windspeed column. This [pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) helped.\n",
    "\n",
    "Another issue I had was the name of the weather station is contained in the metadata. This information is essential for analysis, but it isn’t part of the main data table. I needed a way to extract the station name from the metadata and add it as a column in the dataset. I wasn't going to do this manually.\n",
    "\n",
    "So I wrote a function to extract the station name from the metadata. Each file contains a line like: `Station Name: MACE HEAD` at the start, so the function [searches for that specific line](https://www.statology.org/pandas-query-startswith/) and [grabs the station name](https://www.w3schools.com/python/ref_string_split.asp). To ensure consistency, the name is converted to uppercase and added as a new column `station` in the cleaned dataset. This process ensures that each row of data is tagged with the correct station name for future analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_header_row(file_path):\n",
    "    \"\"\"\n",
    "    Detects the row number where the header starts based on the presence of 'date' and 'wdsp'.\n",
    "    \"\"\"\n",
    "    row_number = 0  # Start counting rows\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:  # Read the file line by line\n",
    "            if \"date\" in line.lower() and \"wdsp\" in line.lower(): # .lower makes it case insensitve just in case\n",
    "                return row_number  # Return the current row number as the header row. This is the row number I can use for skiprows in fucntion below\n",
    "            row_number += 1  # row counter goes up by one\n",
    "    return None  # Return None if no valid header is found\n",
    "\n",
    "\n",
    "def extract_station_name(file_path):\n",
    "    \"\"\"\n",
    "    Extracts the station name from the first line of each csv file that starts with 'Station Name:'.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(\"Station Name:\"):\n",
    "                # Extract the part after \"Station Name:\" and strip any whitespace\n",
    "                return line.split(\"Station Name:\")[1].strip().upper()  # Ensure the name is uppercase\n",
    "    return None  # Return None if no station name is found\n",
    "\n",
    "\n",
    "def read_clean_csv(file_path):\n",
    "    \"\"\"\n",
    "    Reads a CSV file, skips metadata rows, and adds a 'station' column.\n",
    "    \"\"\"\n",
    "    # Extract the station name from the metadata\n",
    "    station_name = extract_station_name(file_path)\n",
    "\n",
    "    # Detect the header row\n",
    "    header_row = detect_header_row(file_path)\n",
    "    if header_row is None:\n",
    "        raise ValueError(f\"No valid header found in {file_path}\")\n",
    "\n",
    "    # Read the data starting from the header row\n",
    "    df = pd.read_csv(file_path, skiprows=header_row, low_memory=False)\n",
    "    \n",
    "    # Add Station Name as a column\n",
    "    df['station'] = station_name\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing fucntions with one of the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Station Name: MACE HEAD\n",
      "Detected Header Row: 17\n",
      "                date  ind rain  ind.1 temp  ind.2 wetb dewpt vappr rhum msl  \\\n",
      "0  13-aug-2003 01:00   -1           4           4                             \n",
      "1  13-aug-2003 02:00   -1           4           4                             \n",
      "2  13-aug-2003 03:00   -1           4           4                             \n",
      "3  13-aug-2003 04:00   -1           4           4                             \n",
      "4  13-aug-2003 05:00   -1           4           4                             \n",
      "\n",
      "   ind.3 wdsp  ind.4 wddir    station  \n",
      "0      7           7        MACE HEAD  \n",
      "1      7           7        MACE HEAD  \n",
      "2      7           7        MACE HEAD  \n",
      "3      7           7        MACE HEAD  \n",
      "4      7           7        MACE HEAD  \n"
     ]
    }
   ],
   "source": [
    "file_path = \"data/hly275/hly275.csv\"  # Replace with the path to one of your files\n",
    "\n",
    "station_name = extract_station_name(file_path)\n",
    "print(f\"Extracted Station Name: {station_name}\")\n",
    "\n",
    "header_row = detect_header_row(file_path)\n",
    "print(f\"Detected Header Row: {header_row}\")\n",
    "\n",
    "cleaned_data = read_clean_csv(file_path)\n",
    "print(cleaned_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making one Big Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like the functions work!\n",
    "\n",
    "I wanted to run the functions with each of the 18 CSV files I had. So I needed to create a list with each of the file paths. But I didn't want to do this manually. The [glob module](https://docs.python.org/3/library/glob.html) helped out here. I was able to search the data directory for the CSV files, and it even searched subdirectories within the data directory using its recursive search function [the `**` pattern](https://www.geeksforgeeks.org/how-to-use-glob-function-to-find-files-recursively-in-python/). So I could keep each CSV file in its own folder alongside its licence and other metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data\\\\hly1075\\\\hly1075.csv', 'data\\\\hly1175\\\\hly1175.csv', 'data\\\\hly1375\\\\hly1375.csv', 'data\\\\hly1475\\\\hly1475.csv', 'data\\\\hly1575\\\\hly1575.csv', 'data\\\\hly1775\\\\hly1775.csv', 'data\\\\hly1875\\\\hly1875.csv', 'data\\\\hly1975\\\\hly1975.csv', 'data\\\\hly2075\\\\hly2075.csv', 'data\\\\hly2175\\\\hly2175.csv', 'data\\\\hly2275\\\\hly2275.csv', 'data\\\\hly2375\\\\hly2375.csv', 'data\\\\hly275\\\\hly275.csv', 'data\\\\hly375\\\\hly375.csv', 'data\\\\hly575\\\\hly575.csv', 'data\\\\hly675\\\\hly675.csv', 'data\\\\hly775\\\\hly775.csv', 'data\\\\hly875\\\\hly875.csv']\n",
      "Number of files found: 18\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# The directory I want to seach and also its subdirectories\n",
    "data_path = \"data/**/*.csv\"  \n",
    "\n",
    "# Get a list of all CSV files in the directory and its subdirectories\n",
    "file_list = glob.glob(data_path, recursive=True)\n",
    "\n",
    "print(file_list)  # Check the list of file paths\n",
    "print(f\"Number of files found: {len(file_list)}\") # Checking I got all of them\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next bit of code processes all the weather station files in the `file_list` by running each through the `read_clean_csv` function, which skips metadata and adds a `station` column to identify the source. The processed DataFrames are stored in the `all_data` list and then combined into a single dataset using [concat](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#concatenating-objects), creating `my_big_dataset` with all the data in one place. I ensured there would be a continuous index with the `ignore_index=True` parameter. \n",
    "\n",
    "Finally, the code checks the number of processed files by counting the entries in `all_data`, confirming that all files were successfully handled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed files: 18\n",
      "Dataset saved as 'my_big_dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "# Process each file in the file_list using the read_clean_csv function\n",
    "all_data = [read_clean_csv(file) for file in file_list]\n",
    "\n",
    "# Combine all processed DataFrames into one\n",
    "my_big_dataset = pd.concat(all_data, ignore_index=True,)\n",
    "\n",
    "# Checking all 18 files were processed\n",
    "print(f\"Number of processed files: {len(all_data)}\")\n",
    "\n",
    "#Saving as CSV file and checking it was saved\n",
    "my_big_dataset.to_csv(\"my_big_dataset.csv\", index=False)\n",
    "print(\"Dataset saved as 'my_big_dataset.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning My Big Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ended up with a dataset that had over 5 million rows. So I printed it out and sat down at the kitchen table with a pencil and ruler so I could check each row for any issues. Just kidding! I didn't do that. Instead I used some handy code to figure out what kind of data I was dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(my_big_dataset)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Checking the structure of the data\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(my_big_dataset\u001b[38;5;241m.\u001b[39minfo())\n",
      "File \u001b[1;32mc:\\Users\\Marcella\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:1214\u001b[0m, in \u001b[0;36mDataFrame.__repr__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buf\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m   1213\u001b[0m repr_params \u001b[38;5;241m=\u001b[39m fmt\u001b[38;5;241m.\u001b[39mget_dataframe_repr_params()\n\u001b[1;32m-> 1214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_string(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrepr_params)\n",
      "File \u001b[1;32mc:\\Users\\Marcella\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Marcella\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:1394\u001b[0m, in \u001b[0;36mDataFrame.to_string\u001b[1;34m(self, buf, columns, col_space, header, index, na_rep, formatters, float_format, sparsify, index_names, justify, max_rows, max_cols, show_dimensions, decimal, line_width, min_rows, max_colwidth, encoding)\u001b[0m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisplay.max_colwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_colwidth):\n\u001b[0;32m   1376\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m fmt\u001b[38;5;241m.\u001b[39mDataFrameFormatter(\n\u001b[0;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1378\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1392\u001b[0m         decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   1393\u001b[0m     )\n\u001b[1;32m-> 1394\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fmt\u001b[38;5;241m.\u001b[39mDataFrameRenderer(formatter)\u001b[38;5;241m.\u001b[39mto_string(\n\u001b[0;32m   1395\u001b[0m         buf\u001b[38;5;241m=\u001b[39mbuf,\n\u001b[0;32m   1396\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   1397\u001b[0m         line_width\u001b[38;5;241m=\u001b[39mline_width,\n\u001b[0;32m   1398\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Marcella\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:962\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_string\u001b[1;34m(self, buf, encoding, line_width)\u001b[0m\n\u001b[0;32m    959\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstring\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringFormatter\n\u001b[0;32m    961\u001b[0m string_formatter \u001b[38;5;241m=\u001b[39m StringFormatter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt, line_width\u001b[38;5;241m=\u001b[39mline_width)\n\u001b[1;32m--> 962\u001b[0m string \u001b[38;5;241m=\u001b[39m string_formatter\u001b[38;5;241m.\u001b[39mto_string()\n\u001b[0;32m    963\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m save_to_buffer(string, buf\u001b[38;5;241m=\u001b[39mbuf, encoding\u001b[38;5;241m=\u001b[39mencoding)\n",
      "File \u001b[1;32mc:\\Users\\Marcella\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\string.py:29\u001b[0m, in \u001b[0;36mStringFormatter.to_string\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_string\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m---> 29\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_string_representation()\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt\u001b[38;5;241m.\u001b[39mshould_show_dimensions:\n\u001b[0;32m     31\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt\u001b[38;5;241m.\u001b[39mdimensions_info\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Marcella\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\string.py:44\u001b[0m, in \u001b[0;36mStringFormatter._get_string_representation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt\u001b[38;5;241m.\u001b[39mframe\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_empty_info_line\n\u001b[1;32m---> 44\u001b[0m strcols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_strcols()\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline_width \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;66;03m# no need to wrap around just print the whole frame\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madj\u001b[38;5;241m.\u001b[39madjoin(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39mstrcols)\n",
      "File \u001b[1;32mc:\\Users\\Marcella\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\string.py:35\u001b[0m, in \u001b[0;36mStringFormatter._get_strcols\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_strcols\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]]:\n\u001b[1;32m---> 35\u001b[0m     strcols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt\u001b[38;5;241m.\u001b[39mget_strcols()\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt\u001b[38;5;241m.\u001b[39mis_truncated:\n\u001b[0;32m     37\u001b[0m         strcols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_dot_separators(strcols)\n",
      "File \u001b[1;32mc:\\Users\\Marcella\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:476\u001b[0m, in \u001b[0;36mDataFrameFormatter.get_strcols\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_strcols\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]]:\n\u001b[0;32m    473\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    Render a DataFrame to a list of columns (as lists of strings).\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 476\u001b[0m     strcols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_strcols_without_index()\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex:\n\u001b[0;32m    479\u001b[0m         str_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_formatted_index(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtr_frame)\n",
      "File \u001b[1;32mc:\\Users\\Marcella\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:740\u001b[0m, in \u001b[0;36mDataFrameFormatter._get_strcols_without_index\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    736\u001b[0m cheader \u001b[38;5;241m=\u001b[39m str_columns[i]\n\u001b[0;32m    737\u001b[0m header_colwidth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcol_space\u001b[38;5;241m.\u001b[39mget(c, \u001b[38;5;241m0\u001b[39m)), \u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madj\u001b[38;5;241m.\u001b[39mlen(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m cheader)\n\u001b[0;32m    739\u001b[0m )\n\u001b[1;32m--> 740\u001b[0m fmt_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_col(i)\n\u001b[0;32m    741\u001b[0m fmt_values \u001b[38;5;241m=\u001b[39m _make_fixed_width(\n\u001b[0;32m    742\u001b[0m     fmt_values, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjustify, minimum\u001b[38;5;241m=\u001b[39mheader_colwidth, adj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madj\n\u001b[0;32m    743\u001b[0m )\n\u001b[0;32m    745\u001b[0m max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madj\u001b[38;5;241m.\u001b[39mlen(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m fmt_values), header_colwidth)\n",
      "File \u001b[1;32mc:\\Users\\Marcella\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:754\u001b[0m, in \u001b[0;36mDataFrameFormatter.format_col\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m    752\u001b[0m frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtr_frame\n\u001b[0;32m    753\u001b[0m formatter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_formatter(i)\n\u001b[1;32m--> 754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m format_array(\n\u001b[0;32m    755\u001b[0m     frame\u001b[38;5;241m.\u001b[39miloc[:, i]\u001b[38;5;241m.\u001b[39m_values,\n\u001b[0;32m    756\u001b[0m     formatter,\n\u001b[0;32m    757\u001b[0m     float_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfloat_format,\n\u001b[0;32m    758\u001b[0m     na_rep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mna_rep,\n\u001b[0;32m    759\u001b[0m     space\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcol_space\u001b[38;5;241m.\u001b[39mget(frame\u001b[38;5;241m.\u001b[39mcolumns[i]),\n\u001b[0;32m    760\u001b[0m     decimal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecimal,\n\u001b[0;32m    761\u001b[0m     leading_space\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex,\n\u001b[0;32m    762\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Marcella\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1161\u001b[0m, in \u001b[0;36mformat_array\u001b[1;34m(values, formatter, float_format, na_rep, digits, space, justify, decimal, leading_space, quoting, fallback_formatter)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     digits \u001b[38;5;241m=\u001b[39m get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisplay.precision\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1147\u001b[0m fmt_obj \u001b[38;5;241m=\u001b[39m fmt_klass(\n\u001b[0;32m   1148\u001b[0m     values,\n\u001b[0;32m   1149\u001b[0m     digits\u001b[38;5;241m=\u001b[39mdigits,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1158\u001b[0m     fallback_formatter\u001b[38;5;241m=\u001b[39mfallback_formatter,\n\u001b[0;32m   1159\u001b[0m )\n\u001b[1;32m-> 1161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fmt_obj\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32mc:\\Users\\Marcella\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1194\u001b[0m, in \u001b[0;36m_GenericArrayFormatter.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_result\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m-> 1194\u001b[0m     fmt_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_strings()\n\u001b[0;32m   1195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _make_fixed_width(fmt_values, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjustify)\n",
      "File \u001b[1;32mc:\\Users\\Marcella\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1259\u001b[0m, in \u001b[0;36m_GenericArrayFormatter._format_strings\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1257\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(vals):\n\u001b[0;32m   1258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m is_float_type[i] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformatter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m leading_space:\n\u001b[1;32m-> 1259\u001b[0m         fmt_values\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_format(v)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1260\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m is_float_type[i]:\n\u001b[0;32m   1261\u001b[0m         fmt_values\u001b[38;5;241m.\u001b[39mappend(float_format(v))\n",
      "File \u001b[1;32mc:\\Users\\Marcella\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1239\u001b[0m, in \u001b[0;36m_GenericArrayFormatter._format_strings.<locals>._format\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mrepr\u001b[39m(x)\n\u001b[0;32m   1237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1238\u001b[0m     \u001b[38;5;66;03m# object dtype\u001b[39;00m\n\u001b[1;32m-> 1239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(formatter(x))\n",
      "File \u001b[1;32mc:\\Users\\Marcella\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\printing.py:165\u001b[0m, in \u001b[0;36mpprint_thing\u001b[1;34m(thing, _nest_lvl, escape_chars, default_escapes, quote_strings, max_seq_items)\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fmt\u001b[38;5;241m.\u001b[39mformat(things\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(pairs))\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpprint_thing\u001b[39m(\n\u001b[0;32m    166\u001b[0m     thing: Any,\n\u001b[0;32m    167\u001b[0m     _nest_lvl: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    168\u001b[0m     escape_chars: EscapeChars \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    169\u001b[0m     default_escapes: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    170\u001b[0m     quote_strings: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    171\u001b[0m     max_seq_items: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    172\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    173\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m    This function is the sanctioned way of converting objects\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;124;03m    to a string representation and properly handles nested sequences.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;124;03m    str\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mas_escaped_string\u001b[39m(\n\u001b[0;32m    197\u001b[0m         thing: Any, escape_chars: EscapeChars \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m escape_chars\n\u001b[0;32m    198\u001b[0m     ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(my_big_dataset)\n",
    "\n",
    "# Checking the structure of the data\n",
    "print(my_big_dataset.info())\n",
    "\n",
    "# Check for missing values\n",
    "print(my_big_dataset.isnull().sum())\n",
    "\n",
    "# Check unique values in a column\n",
    "print(my_big_dataset['WindSpeed'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Up Missing Values\n",
    "\n",
    "I focused on cleaning up the missing values in the dataset. Most of the original datasets had 16 columns, but a few included 5 additional columns. This introduced missing values when the datasets were merged, so it made sense that the last five columns had nearly 4 million missing values. Fortunately, `pandas.concat` [automatically converted](https://pyimagesearch.com/2024/05/07/using-pandas-concat-pd-concat/) these blanks into proper missing values (`NaN`), which were easy to handle during analysis.\n",
    "\n",
    "However, when I inspected the other columns, `pandas` was reporting 0 missing values, which didn’t match what I saw when scrolling through `my_big_dataset.csv`. The issue was that some missing values were misrepresented as empty strings (`\"\"`). They weren’t being recognised as missing values but as blank text, and this would cause problems during analysis. \n",
    "\n",
    "Another related issue was that most of the columns containing integers were [being read as `object` type](https://www.geeksforgeeks.org/pandas-detect-mixed-data-types-and-fix-it/). This happened because `pandas` was trying to handle the mix of integers and empty strings in those columns. \n",
    "\n",
    "To fix this, I replaced all empty strings with `NaN` using [`replace`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html) and ensured the columns had the correct data types, making the dataset consistent and ready for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date             0\n",
      "ind              0\n",
      "rain             0\n",
      "ind.1            0\n",
      "temp             0\n",
      "ind.2            0\n",
      "wetb             0\n",
      "dewpt            0\n",
      "vappr            0\n",
      "rhum             0\n",
      "msl              0\n",
      "ind.3            0\n",
      "wdsp             0\n",
      "ind.4            0\n",
      "wddir            0\n",
      "station          0\n",
      "ww         3828149\n",
      "w          3828149\n",
      "sun        3828149\n",
      "vis        3828149\n",
      "clht       3828149\n",
      "clamt      3828149\n",
      "dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5745375 entries, 0 to 5745374\n",
      "Data columns (total 22 columns):\n",
      " #   Column   Dtype         \n",
      "---  ------   -----         \n",
      " 0   date     datetime64[ns]\n",
      " 1   ind      int64         \n",
      " 2   rain     object        \n",
      " 3   ind.1    int64         \n",
      " 4   temp     object        \n",
      " 5   ind.2    int64         \n",
      " 6   wetb     object        \n",
      " 7   dewpt    object        \n",
      " 8   vappr    object        \n",
      " 9   rhum     object        \n",
      " 10  msl      object        \n",
      " 11  ind.3    int64         \n",
      " 12  wdsp     object        \n",
      " 13  ind.4    int64         \n",
      " 14  wddir    object        \n",
      " 15  station  object        \n",
      " 16  ww       object        \n",
      " 17  w        object        \n",
      " 18  sun      object        \n",
      " 19  vis      object        \n",
      " 20  clht     object        \n",
      " 21  clamt    object        \n",
      "dtypes: datetime64[ns](1), int64(5), object(16)\n",
      "memory usage: 964.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Replace all empty strings with NaN in the dataset\n",
    "my_big_dataset.replace(\"\", np.nan, inplace=True)\n",
    "\n",
    "# Verify the change by checking for missing values\n",
    "print(my_big_dataset.isnull().sum())\n",
    "print(my_big_dataset.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also converted the `date` column to a proper datetime format for easier analysis and consistency. It was taking forever to convert to datetime. I stopped running the code after 10mins. So I went googling and I found [this thread](https://stackoverflow.com/questions/32034689/why-is-pandas-to-datetime-slow-for-non-standard-time-format-such-as-2014-12-31) that expained the it would be quicker if I gave pandas the format for the date column - so it wouldn't have to go looking for it and it worked. It did 5 million rows in 1m29s!\n",
    "\n",
    "And just to be sure I got rid of duplicate rows just to be safe and ensure all data is unique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5745375 entries, 0 to 5745374\n",
      "Data columns (total 22 columns):\n",
      " #   Column   Dtype         \n",
      "---  ------   -----         \n",
      " 0   date     datetime64[ns]\n",
      " 1   ind      int64         \n",
      " 2   rain     object        \n",
      " 3   ind.1    int64         \n",
      " 4   temp     object        \n",
      " 5   ind.2    int64         \n",
      " 6   wetb     object        \n",
      " 7   dewpt    object        \n",
      " 8   vappr    object        \n",
      " 9   rhum     object        \n",
      " 10  msl      object        \n",
      " 11  ind.3    int64         \n",
      " 12  wdsp     object        \n",
      " 13  ind.4    int64         \n",
      " 14  wddir    object        \n",
      " 15  station  object        \n",
      " 16  ww       object        \n",
      " 17  w        object        \n",
      " 18  sun      object        \n",
      " 19  vis      object        \n",
      " 20  clht     object        \n",
      " 21  clamt    object        \n",
      "dtypes: datetime64[ns](1), int64(5), object(16)\n",
      "memory usage: 964.3+ MB\n",
      "None\n",
      "                 date  ind rain  ind.1  temp  ind.2  wetb dewpt vappr rhum  \\\n",
      "0 1955-12-01 01:00:00    0  0.0      0  10.7      0  10.0   9.4  11.8   91   \n",
      "1 1955-12-01 02:00:00    0  2.9      0   9.8      0   9.7  10.0  12.0   99   \n",
      "2 1955-12-01 03:00:00    0  3.8      0   9.7      0   9.5   9.4  11.7   97   \n",
      "3 1955-12-01 04:00:00    0  0.8      0   9.8      0   9.7   9.4  11.9   98   \n",
      "4 1955-12-01 05:00:00    0  0.3      0   8.9      0   8.7   8.3  11.1   97   \n",
      "\n",
      "   ... wdsp  ind.4 wddir       station   ww    w  sun  vis clht clamt  \n",
      "0  ...   16      1   170  ROCHES POINT  NaN  NaN  NaN  NaN  NaN   NaN  \n",
      "1  ...   11      1   190  ROCHES POINT  NaN  NaN  NaN  NaN  NaN   NaN  \n",
      "2  ...    9      1   160  ROCHES POINT  NaN  NaN  NaN  NaN  NaN   NaN  \n",
      "3  ...    5      1   140  ROCHES POINT  NaN  NaN  NaN  NaN  NaN   NaN  \n",
      "4  ...   12      1   330  ROCHES POINT  NaN  NaN  NaN  NaN  NaN   NaN  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "date             0\n",
      "ind              0\n",
      "rain             0\n",
      "ind.1            0\n",
      "temp             0\n",
      "ind.2            0\n",
      "wetb             0\n",
      "dewpt            0\n",
      "vappr            0\n",
      "rhum             0\n",
      "msl              0\n",
      "ind.3            0\n",
      "wdsp             0\n",
      "ind.4            0\n",
      "wddir            0\n",
      "station          0\n",
      "ww         3828149\n",
      "w          3828149\n",
      "sun        3828149\n",
      "vis        3828149\n",
      "clht       3828149\n",
      "clamt      3828149\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert the date column to datetime\n",
    "my_big_dataset['date'] = pd.to_datetime(my_big_dataset['date'], format='%d-%b-%Y %H:%M')\n",
    "\n",
    "# Drop duplicate rows\n",
    "my_big_dataset.drop_duplicates(inplace=True)\n",
    "\n",
    "# Check the cleaned data\n",
    "print(my_big_dataset.info())\n",
    "print(my_big_dataset.head())\n",
    "print(my_big_dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_columns\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# Show all columns\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Display the dataset\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m(claremorris_data)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Reset display options to default after viewing\u001b[39;00m\n\u001b[0;32m     13\u001b[0m pd\u001b[38;5;241m.\u001b[39mreset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_rows\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Marcella\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:1214\u001b[0m, in \u001b[0;36mDataFrame.__repr__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buf\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m   1213\u001b[0m repr_params \u001b[38;5;241m=\u001b[39m fmt\u001b[38;5;241m.\u001b[39mget_dataframe_repr_params()\n\u001b[1;32m-> 1214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_string(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrepr_params)\n",
      "File \u001b[1;32mc:\\Users\\Marcella\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Marcella\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:1394\u001b[0m, in \u001b[0;36mDataFrame.to_string\u001b[1;34m(self, buf, columns, col_space, header, index, na_rep, formatters, float_format, sparsify, index_names, justify, max_rows, max_cols, show_dimensions, decimal, line_width, min_rows, max_colwidth, encoding)\u001b[0m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisplay.max_colwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_colwidth):\n\u001b[0;32m   1376\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m fmt\u001b[38;5;241m.\u001b[39mDataFrameFormatter(\n\u001b[0;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1378\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1392\u001b[0m         decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   1393\u001b[0m     )\n\u001b[1;32m-> 1394\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fmt\u001b[38;5;241m.\u001b[39mDataFrameRenderer(formatter)\u001b[38;5;241m.\u001b[39mto_string(\n\u001b[0;32m   1395\u001b[0m         buf\u001b[38;5;241m=\u001b[39mbuf,\n\u001b[0;32m   1396\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   1397\u001b[0m         line_width\u001b[38;5;241m=\u001b[39mline_width,\n\u001b[0;32m   1398\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Marcella\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:962\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_string\u001b[1;34m(self, buf, encoding, line_width)\u001b[0m\n\u001b[0;32m    959\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mformats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstring\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringFormatter\n\u001b[0;32m    961\u001b[0m string_formatter \u001b[38;5;241m=\u001b[39m StringFormatter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt, line_width\u001b[38;5;241m=\u001b[39mline_width)\n\u001b[1;32m--> 962\u001b[0m string \u001b[38;5;241m=\u001b[39m string_formatter\u001b[38;5;241m.\u001b[39mto_string()\n\u001b[0;32m    963\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m save_to_buffer(string, buf\u001b[38;5;241m=\u001b[39mbuf, encoding\u001b[38;5;241m=\u001b[39mencoding)\n",
      "File \u001b[1;32mc:\\Users\\Marcella\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\string.py:29\u001b[0m, in \u001b[0;36mStringFormatter.to_string\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_string\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m---> 29\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_string_representation()\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt\u001b[38;5;241m.\u001b[39mshould_show_dimensions:\n\u001b[0;32m     31\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt\u001b[38;5;241m.\u001b[39mdimensions_info\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Marcella\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\string.py:51\u001b[0m, in \u001b[0;36mStringFormatter._get_string_representation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madj\u001b[38;5;241m.\u001b[39madjoin(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39mstrcols)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_need_to_wrap_around:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_join_multiline(strcols)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_strcols_to_terminal_width(strcols)\n",
      "File \u001b[1;32mc:\\Users\\Marcella\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\string.py:149\u001b[0m, in \u001b[0;36mStringFormatter._join_multiline\u001b[1;34m(self, strcols_input)\u001b[0m\n\u001b[0;32m    147\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m             row\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m nrows)\n\u001b[1;32m--> 149\u001b[0m     str_lst\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madj\u001b[38;5;241m.\u001b[39madjoin(adjoin_width, \u001b[38;5;241m*\u001b[39mrow))\n\u001b[0;32m    150\u001b[0m     start \u001b[38;5;241m=\u001b[39m end\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(str_lst)\n",
      "File \u001b[1;32mc:\\Users\\Marcella\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\printing.py:525\u001b[0m, in \u001b[0;36m_TextAdjustment.adjoin\u001b[1;34m(self, space, *lists, **kwargs)\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madjoin\u001b[39m(\u001b[38;5;28mself\u001b[39m, space: \u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m*\u001b[39mlists, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m--> 525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m adjoin(space, \u001b[38;5;241m*\u001b[39mlists, strlen\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlen, justfunc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjustify, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Marcella\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\printing.py:56\u001b[0m, in \u001b[0;36madjoin\u001b[1;34m(space, *lists, **kwargs)\u001b[0m\n\u001b[0;32m     54\u001b[0m maxLen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mlen\u001b[39m, lists))\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, lst \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lists):\n\u001b[1;32m---> 56\u001b[0m     nl \u001b[38;5;241m=\u001b[39m justfunc(lst, lengths[i], mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     57\u001b[0m     nl \u001b[38;5;241m=\u001b[39m ([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m lengths[i]] \u001b[38;5;241m*\u001b[39m (maxLen \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(lst))) \u001b[38;5;241m+\u001b[39m nl\n\u001b[0;32m     58\u001b[0m     newLists\u001b[38;5;241m.\u001b[39mappend(nl)\n",
      "File \u001b[1;32mc:\\Users\\Marcella\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\printing.py:518\u001b[0m, in \u001b[0;36m_TextAdjustment.justify\u001b[1;34m(self, texts, max_len, mode)\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;124;03mPerform ljust, center, rjust against string or list-like\u001b[39;00m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [x\u001b[38;5;241m.\u001b[39mljust(max_len) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcenter\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [x\u001b[38;5;241m.\u001b[39mcenter(max_len) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m texts]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Filter rows for Claremorris station\n",
    "claremorris_data = my_big_dataset[my_big_dataset['station'] == 'CLAREMORRIS']\n",
    "\n",
    "\n",
    "# Temporarily set display options to show all rows and columns\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "\n",
    "# Display the dataset\n",
    "print(claremorris_data)\n",
    "\n",
    "# Reset display options to default after viewing\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_columns')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
